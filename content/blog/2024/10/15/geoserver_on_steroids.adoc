= GeoServer on steroids
Stefan Ziegler
2024-10-15
:jbake-type: post
:jbake-status: published
:jbake-tags: Java, GeoServer, GraalVM, GeoScript
:idprefix:

https://blog.sogeo.services/tags/GraalVM.html[Ich] mag https://www.graalvm.org/[_GraalVM_]. Sei es um Java-Anwendungen zu einem Native Image runterzukompilieren oder Python mit Java zu verheiraten. Ein weiterer spannender Punkt ist der eigene Graal JIT Compiler. Dieser kann zu einer Performancesteigerung der Java-Anwendung führen und zwar ohne, dass man an dieser überhaupt was ändern muss. Nur die Runtime (&laquo;Java-Variante&raquo;) austauschen und gut ist. So hat der Wechsel zum Graal JIT Compiler z.B. bei der https://www.morling.dev/blog/one-billion-row-challenge/[1BRC] eine https://x.com/gunnarmorling/status/1843649474545287202/photo/3[Performancesteigerung von acht Prozent] gebracht. Bei mir selber habe ich solche Unterschiede noch keine festgestellt. Vor https://blog.sogeo.services/blog/2021/11/28/interlis-leicht-gemacht-number-27.html[ein paar Jahren] habe ich mit https://github.com/claeis/ilivalidator[_ilivalidator_] und https://github.com/claeis/ili2db[_ili2pg_] ein paar Benchmarks gemacht, die aber nicht sonderlich aussagekräftig waren und nicht gezeigt hätten, dass _GraalVM_ bedeutend schneller ist. Weil wir uns zur Zeit überlegen von _QGIS Server_ auf https://geoserver.org[_GeoServer_] zu wechseln, wäre das jedenfalls ein Anwendungfall, den man untersuchen könnte, da der Server zu Bürozeiten relativ stark beansprucht wird und somit genügend Dauerlast produziert wird. Obwohl wir relativ viele Java-Anwendungen in Betrieb haben, hat keine davon eine ähnliche Last zu tragen wie der Kartendienst.

Jetzt benötigte ich ein Testszenario, das aus mehr als bloss vier Flächen und zwei Punkten besteht. Ich habe mich entschieden die Nutzungsplanung und ein Orthofoto (swissimage 2021) zu verwenden. Die Nutzungsplanung steht als Vektordatensatz in einzelnen INTERLIS-Transferdateien, die ich https://github.com/edigonzales/geoserver-benchmarks/blob/461afff02f2b9bf1e96dd9339eb39ddccc2a95da/gretl/build.gradle[ruckzuck] mit https://gretl.app[_GRETL_] in eine gedockerte PostgreSQL/PostGIS-Datenbank importieren kann. Die SLD-Datei hatte ich noch von früheren Spielereien rumliegen:

image::../../../../../images/geoserver_on_steroids/npl_wms.png[alt="WMS-Bild Nutzungsplanung", align="center"]

Das Orthofoto liegt als 30 GB grosses cloud optimized GeoTIFF _lokal_ vor:

image::../../../../../images/geoserver_on_steroids/ortho_wms.png[alt="WMS-Bild Orthofoto", align="center"]

Wie bei meinen anderen Service-Benchmarks habe ich https://jmeter.apache.org/[_jMeter_] verwendet. Hat man es halbwegs durchschaut und hat noch alte Konfigurationen rumliegen, ist es durchaus halb-angenehm damit zu arbeiten. Der wichtige Konfigurationartefakt von _jMeter_ ist die CSV-Datei mit den unterschiedlichen Werten zu Width, Height, Boundingbox und Layer. In _jMeter_ definiere ich die URL zum WMS-Server mit Platzhaltern für besagte Werte. _JMeter_ ersetzt nun bei jedem Request diese Platzhalter mit den Werten aus der CSV-Datei:

[source,bash,linenums]
----
width;height;bbox;layer
2102;2119;2605738.274269104,1236815.4005975279,2605773.134554806,1236850.5428170345;ch.swisstopo.swissimage_2021.rgb
3308;1924;2619968.8471599706,1256153.9242204945,2632686.446437572,1263550.7383106109;ch.swisstopo.swissimage_2021.rgb
3320;1896;2610788.4627983514,1256404.5859801334,2617948.8869838137,1260493.7920812287;ch.swisstopo.swissimage_2021.rgb
3470;1797;2634212.7299694223,1248533.8556387443,2641987.561823341,1252560.1878697216;ch.swisstopo.swissimage_2021.rgb
1972;2126;2592921.414637569,1242392.7718350205,2601610.971095325,1251760.9244867398;nutzungsplanung_grundnutzung
3391;1803;2597536.2772151744,1211156.517151424,2632904.225935421,1229961.7107646957;ch.swisstopo.swissimage_2021.rgb
3551;1134;2604904.606863846,1237243.4070413096,2623471.839928261,1243172.7909599373;nutzungsplanung_grundnutzung
...
----

Wie habe ich die CSV-Datei hergestellt? Als Grundlage diente mir ein https://github.com/edigonzales-dumpster/geoserver-tests/blob/35e7010a6ca6eb246c4d5612b23c269904ed1afc/benchmark/scripts/wms_request.py[Python-Skript] von den legendären https://wiki.osgeo.org/wiki/FOSS4G_Benchmark[FOSS4G Performance Shoot-outs]. Das Skript hat verschiedene Parameter mit dem man den Output steuern kann: So ist es möglich einen Range für Width und Height anzugeben und die übergeordnete Region (als Bounding Box). Zusätzlich lässt sich ein Auflösungsrange angeben, was nichts anderes ist als Massstabsbereiche. Als letztes Zückerchen kann man eine Shapedatei angeben, um sicherzustellen, dass die berechnete Bounding Box entweder komplett innerhalb der Shapefilegeometrien liegt oder mindestes schneidet. Für diese Geometrieoperationen verwendet das Python-Skript Bindings für OGR/GDAL. Das war mir ein (Installations-)Graus. Ich habe das Skript _ChatGPT_ geschickt mit der Bitte um eine Umwandlung nach https://github.com/geoscript/geoscript-groovy[GeoScript Groovy]. Das hat sogar https://github.com/edigonzales/geoserver-benchmarks/blob/e7ee9c96372d67a0db8b862300fab824fdd99df6/scripts/wms_requests.groovy[halbwegs gut] funktioniert und weil es für GeoScript eine https://jericks.github.io/geoscript-groovy-cookbook/#uber-jar[Uber Jar] gibt, ist die Installation nur ein Download dieser Jar-Datei. Der Aufruf des Skriptes ist simpel: `java -jar geoscript-groovy-app-1.22.0.jar script wms_requests.groovy`. Maximal so kompliziert sollte &laquo;geo-scripting&raquo; sein. Dünkt mich angenehmer als Python-Bindings installieren und auf Teufel komm raus die passenden OGR/GDAL-Libs (mit _Conda_ oder dergleichen).
 




https://www.youtube.com/watch?v=ZiqWi6SpqOg[&laquo;Talent borrows, Genius steals&raquo;]: Das https://www.zh.ch/de/direktion-der-justiz-und-des-innern/statistisches-amt.html[Statistische Amt] des Kantons Zürich hat einen sehr interessanten https://github.com/machinelearningZH/simply-simplify-language[Prototypen] zur Vereinfachung von Texten in https://www.edi.admin.ch/dam/edi/de/dokumente/gleichstellung/E-Accessibility/empfehlungen_informationen_leichtesprache_gebaerdensprache.pdf.download.pdf/Empfehlungen%20f%C3%BCr%20Verwaltungen%20zur%20Erstellung%20von%20Informationen%20in%20Leichter%20Sprache%20und%20Geb%C3%A4rdensprache.pdf[einfache und leichte] Sprache entwickelt. (Meine Eselsbrücke: Die leichte Sprache ist noch einfacher, als die einfache Sprache.) Für mich war klar, das will ich auch ausprobieren. Was mich jedoch besonders interessiert, ist, wie gut ein frei verfügbares LLM (_Llama3_) die Aufgabe meistert. Das Ganze hat sich natürlich aufwändiger als geplant herausgestellt. Aber der Reihe nach:

Der ZH-Prototyp ist mit Python und https://streamlit.io/[_Streamlit_] umgesetzt. Ich habe mit beidem wenig bis keine Erfahrung und darum dachte ich, dass ich den GUI-Teil schnell mit https://vaadin.com/[_Vaadin_] umsetzen kann. Das eigentlich GUI ging tatsächlich schnell, es stellte sich jedoch heraus, dass die Anwendung auch noch einen beträchtlichen Teil an Businesslogik beinhaltet: Nämlich die Analyse resp. Bewertung der Texte. Letzten Endes läuft es auf die Einstufung der Texte (vorher/nachher) in ein https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages[CEFR-Level] hinaus. Die Anwendung verwendet https://spacy.io/[_spaCy_], eine NLP-Bibliothek, um https://github.com/machinelearningZH/simply-simplify-language/blob/main/_streamlit_app/sprache-vereinfachen.py#L153[bestimmte Parameter] zu berechnen. Mit diesen Parametern wird anschliessend die https://github.com/machinelearningZH/simply-simplify-language/blob/main/_streamlit_app/sprache-vereinfachen.py#L232[Verständlichkeit] eines Textes berechnet. Die Formel haben sie mittels eines Logistic Regression Modelles hergeleitet. Dieses spaCy-Teil gibt es natürlich nicht 1:1 in Java. Ich habe mit https://dev.languagetool.org/[languagetool] eine Java-Bibliothek gefunden, die ähnliche Funktionen aufweist. Verschiedene Parameter weisen beim Ausprobieren die exakt gleichen Werte aus, andere - z.B. der https://github.com/machinelearningZH/simply-simplify-language/blob/main/_streamlit_app/sprache-vereinfachen.py#L161[common word score] - nicht. Soweit ich es nachvollziehen kann, liegt es daran, dass unterschiedlich &laquo;lemmatized&raquo; wird. Das führt natürlich zur Frage, ob die erwähnte Formel auch in meinem Stack stimmt. Wahrscheinlich weniger gut, als im Original. Trotzdem führt es meines Erachtens zu plausiblen und sinnvollen (leicht unterschiedlichen) Punktzahlen.

Neben dieser Analyse-Logik hat der Kanton Zürich sicher auch einiges an Zeit in die https://github.com/edigonzales/simply-simplify-language-java/tree/main/src/main/resources/prompts[Prompts] investiert, die für die guten Resultat unabdingbar sind. Auch hier: Ehre, wem Ehre gebührt.

Mein Prototyp mit Vaadin und einem Resultat mit GPT-4o:

image::../../../../../images/ai_behoerdensprache_ade_p2/prototyp01.png[alt="prototyp01", align="center"]

Die Resultate dünken mich ziemlich gut. Mit GPT-4o scheint mir schneller als GPT-4 zu sein und häufiger mit Aufzählungn zu arbeiten als GPT-4.

Wie sieht es mit _Llama3_ aus? Die Herausforderung war eine brauchbare Testumgebung zu finden. Lokal mit meinem M1-Prozessor macht es wirklich gar keinen Spass. Ein Mitarbeiter hat auf seinem Gamer-PC mit einer _RTX 4080 Super_ Grafikkarte gezeigt, dass die (gefühlte) Geschwindigkeit nahe an ChatGPT 4 ist. Machte ich mich also auf die Suche nach einem Cloud-Anbieter, der GPU-Rechner im Programm hat. Die wirklich grossen Hyperscaler waren mir entweder zu kompliziert (Azure, weil noch nie was gemacht mit) oder dermassen undurchschaubar im Angebot (AWS), dass ich nicht wusste, welche Instanz nun geeignet ist für meine Tests. Hetzner hat zwar einen GPU-Rechner im Angebot, jedoch mit hohen Setup-Gebühren. OVHCloud hätte zwar viele unterschiedliche GPU-Rechner im Angebot. Als Neukunde kann man diese jedoch, trotz hinterlegter Kreditkarte, nicht verwenden. Eine Anfrage wurde abgelehnt. Keine Ahnung warum. Als halber Freund von DigitalOcean bin ich über Paperspace gestolpert. Die wurden vor kurzem von DigitalOcean aufgekauft und haben GPU-Rechner im Angebot. Auch hier muss man sich mittels Ticket die Rechner freischalten lassen aber danach kann man frisch fröhlich verschiedenste GPU-Rechner auswählen. 

Ich habe mich für einen Rechner mit einer Ampere A4000 Nvidia Karte für $0.76/h entschieden. Die richtige Coolen würden A100-80Gx8 für $25.44/h wählen... 

_Ollama_ installieren geht eigentlich ganz passabel, es installiert jedoch einen neuen Kernel (soweit ich es verstanden habe) und dauert darum ein Weilchen und die VM muss gerebootet werden. Anschliessend reicht:

[source,bash,linenums]
----
ollama run llama3
----

Das startet _Ollama_ mit _Llama3_ als LLM. Man muss _Ollama_ beibringen, dass er nicht bloss auf 127.0.0.1 hören soll, sondern auf allen lokalen Interfaces. Dazu muss die Env-Variable `OLLAMA_HOST=0.0.0.0` in der Datei _/etc/systemd/system/ollama.service_ gesetzt werden. _Ollama_ ist nun auch von extern ansprechbar (so natürlich nie in Produktion gehen). Prüfen kann man es mit curl:

[source,bash,linenums]
----
curl http://74.82.28.177:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?"
}'
----

Die Geschwindigkeit kann sich wirklich https://youtu.be/V87j4nev-_Q[sehen lassen]. 

Wie gut funktioniert es aber mit unserem Behördensprache-ade-Tool? Die Geschwindigkeit ist trotz umfangreicherer Prompts auf GPT-4o-Niveau. Die Qualität der vereinfachten Texte ist jedoch weniger gut resp. sie erreichen nur Punktzahlen zwischen 10 und 15 (der Ausgangstext hat eine Punktzahl von 1). Das ist zwar ein wenig ernüchternd aber soweit ich es nachvollziehen konnte, ist _Llama3_ nicht wirklich auf die deutsche Sprache trainiert. Es gibt auf HuggingFace Llama3-basierte Modelle, die mit https://huggingface.co/DiscoResearch/Llama3-German-8B[deutschen Token] feingetuned wurden. Ob mit einem solchen Modell bessere Ergebnisse erzielt werden können, müsste man noch ausprobieren.

image::../../../../../images/ai_behoerdensprache_ade_p2/prototyp02.png[alt="prototyp02", align="center"]

Nichtsdestotrotz finde ich es wichtig, dass es frei verfügbare LLM gibt. Für Organisationen werden sie spannend, wenn man &laquo;KI machen&raquo; will und keine Kompromisse beim Datenschutz und der digitalen Souveränität eingehen will/kann. 

Ausprobieren:

[source,bash,linenums]
----
docker run -p8080:8080 -e OPENAI_API_KEY=123456789 edigonzales/simply-simplify-language
----

Für _Llama3_ müssen die https://github.com/edigonzales/simply-simplify-language-java/blob/main/src/main/resources/application.properties#L17[zwei Env-Variablen] gesetzt werden.

Links:

 - Original: https://github.com/machinelearningZH/simply-simplify-language/
 - Shameless plug: https://github.com/edigonzales/simply-simplify-language-java/